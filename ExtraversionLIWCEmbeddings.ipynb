{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "import os, codecs\n",
    "%matplotlib inline\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics as skmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 11:30:02,284 : INFO : loading Word2Vec object from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model\n",
      "2017-05-03 11:30:09,433 : INFO : loading wv recursively from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model.wv.* with mmap=None\n",
      "2017-05-03 11:30:09,433 : INFO : loading syn0 from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model.wv.syn0.npy with mmap=None\n",
      "2017-05-03 11:30:17,298 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-03 11:30:17,326 : INFO : loading syn1neg from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model.syn1neg.npy with mmap=None\n",
      "2017-05-03 11:30:56,800 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-03 11:30:56,911 : INFO : loaded /home/bahbbc/workspace/masters-big5/models/tweet50-600.model\n",
      "2017-05-03 11:32:27,656 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('/home/bahbbc/workspace/masters-big5/models/tweet50-600.model')\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, 186)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('~/personality-normalized-word2vec-norm.csv', encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_w2v_data, test_w2v_data = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(vectorizer, classifier, data):\n",
    "    data_features = vectorizer.transform(data['formatted_text'])\n",
    "    predictions = classifier.predict(data_features)\n",
    "    target = data['extraversion_m']\n",
    "    evaluate_prediction(predictions, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    #print words.shape\n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.wv.vocab:\n",
    "            mean.append(wv.wv.syn0norm[wv.wv.vocab[word].index])\n",
    "            all_words.add(wv.wv.vocab[word].index)\n",
    "        #print mean\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(num_features,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    if text is np.nan:\n",
    "        return []\n",
    "    for sent in nltk.sent_tokenize(text, language='portuguese'):\n",
    "        for word in nltk.word_tokenize(sent, language='portuguese'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokenized = test_w2v_data.apply(lambda r: w2v_tokenize_text(r['formatted_text']), axis=1).values\n",
    "train_tokenized = train_w2v_data.apply(lambda r: w2v_tokenize_text(r['formatted_text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 11:37:10,349 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:11,083 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:11,092 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:12,166 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:12,546 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:13,217 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:14,312 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:14,435 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:14,775 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:15,007 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 11:37:15,083 : WARNING : cannot compute similarity with no input []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.88 s, sys: 327 ms, total: 6.21 s\n",
      "Wall time: 8.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(model,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC Attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "personalities = pd.read_csv('personality-normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "personalities = personalities.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    529\n",
       "1    510\n",
       "Name: extraversion_m, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personalities.extraversion_m.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(personalities, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liwc_attr = ['X1funct' ,'X2pronoun' ,'X3ppron','X4i','X5we','X6you','X7shehe','X8they','X9ipron','X10article','X11verb','X12auxverb','X13past','X14present','X15future','X16adverb','X17preps','X18conj','X19negate','X20quant','X21number','X22swear','X23social','X24family','X25friend','X26humans','X27affect','X28posemo','X29negemo','X30anx','X31anger','X32sad','X33cogmech','X34insight','X35cause','X36discrep','X37tentat','X38certain','X39inhib','X40incl','X41excl','X42percept','X43see','X44hear','X45feel','X46bio','X47body','X48health','X49sexual','X50ingest','X51relativ','X52motion','X53space','X54time','X55work','X56achieve','X57leisure','X58home','X59money','X60relig','X61death','X62assent','X63nonfl', 'X64filler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nan(atributes):\n",
    "    train_attributes = np.array(train_data[atributes])\n",
    "    test_attributes = np.array(test_data[atributes])\n",
    "    train_attributes[np.isnan(train_attributes)] = 0\n",
    "    test_attributes[np.isnan(test_attributes)] = 0\n",
    "    return (train_attributes, test_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = remove_nan(liwc_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionairies + word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(727, 64)\n",
      "(727, 600)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print X_train_word_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_attributes(conj1, conj2):\n",
    "    combined_attr = []\n",
    "\n",
    "    for i in range(0,conj1.shape[0]):\n",
    "        mixed_attr = np.concatenate((conj1[i], conj2[i]))\n",
    "        combined_attr.append(mixed_attr)\n",
    "    return np.vstack(combined_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_merged = merge_attributes(train, X_train_word_average)\n",
    "test_merged = merge_attributes(test, X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "force = False\n",
    "model_trainer = RandomizedSearchCV(\n",
    "    n_iter=1, \n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_distributions={\n",
    "        \"criterion\": [\"gini\"],\n",
    "        \"n_estimators\": [1000],\n",
    "        \"max_features\": [\"log2\"],\n",
    "        \"max_depth\": [None],\n",
    "        \"bootstrap\": [True],\n",
    "        \"oob_score\": [True],\n",
    "        \"class_weight\": [\"balanced\"],\n",
    "        \"random_state\": [42]\n",
    "    },\n",
    "    scoring=\"f1\",\n",
    "    verbose=True,\n",
    "    refit=True,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   27.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.49 s, sys: 495 ms, total: 5.99 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_trainer.fit(train_merged, train_w2v_data['extraversion_m'])\n",
    "model = model_trainer.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = model.predict(test_merged)\n",
    "yt = test_w2v_data['extraversion_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y_pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y_pred    0   1\n",
       "y_true         \n",
       "0       106  36\n",
       "1        82  88"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    index=pd.Index([0, 1], name=\"y_true\"),\n",
    "    columns=pd.Index([0, 1], name=\"y_pred\"),\n",
    "    data=skmetrics.confusion_matrix(y_true=yt, y_pred=yp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.75      0.64       142\n",
      "          1       0.71      0.52      0.60       170\n",
      "\n",
      "avg / total       0.64      0.62      0.62       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print skmetrics.classification_report(y_true=yt, y_pred=yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62179487179487181"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yt, yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 12:14:17,627 : INFO : loading Word2Vec object from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model\n",
      "2017-05-03 12:14:24,035 : INFO : loading wv recursively from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model.wv.* with mmap=None\n",
      "2017-05-03 12:14:24,035 : INFO : loading syn0 from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model.wv.syn0.npy with mmap=None\n",
      "2017-05-03 12:14:31,866 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-03 12:14:31,867 : INFO : loading syn1neg from /home/bahbbc/workspace/masters-big5/models/tweet50-600.model.syn1neg.npy with mmap=None\n",
      "2017-05-03 12:15:07,127 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-03 12:15:07,337 : INFO : loaded /home/bahbbc/workspace/masters-big5/models/tweet50-600.model\n",
      "2017-05-03 12:16:36,943 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('/home/bahbbc/workspace/masters-big5/models/tweet50-600.model')\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    if text is np.nan:\n",
    "        return []\n",
    "    for sent in nltk.sent_tokenize(text, language='portuguese'):\n",
    "        for word in nltk.word_tokenize(sent, language='portuguese'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word in stopwords.words('portuguese'):\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tokenized = train_w2v_data.apply(lambda r: w2v_tokenize_text(r['formatted_text']), axis=1).values\n",
    "test_tokenized = test_w2v_data.apply(lambda r: w2v_tokenize_text(r['formatted_text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 12:28:31,412 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:31,971 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:31,979 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:32,803 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:33,098 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:33,551 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:34,386 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:34,482 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:34,728 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:34,915 : WARNING : cannot compute similarity with no input []\n",
      "2017-05-03 12:28:34,973 : WARNING : cannot compute similarity with no input []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 s, sys: 78.4 ms, total: 3.9 s\n",
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(model,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.62 s, sys: 340 ms, total: 5.96 s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_trainer.fit(X_train_word_average, train_w2v_data['extraversion_m'])\n",
    "model = model_trainer.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = model.predict(X_test_word_average)\n",
    "yt = test_w2v_data['extraversion_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y_pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y_pred    0   1\n",
       "y_true         \n",
       "0       100  42\n",
       "1        82  88"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    index=pd.Index([0, 1], name=\"y_true\"),\n",
    "    columns=pd.Index([0, 1], name=\"y_pred\"),\n",
    "    data=skmetrics.confusion_matrix(y_true=yt, y_pred=yp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.70      0.62       142\n",
      "          1       0.68      0.52      0.59       170\n",
      "\n",
      "avg / total       0.62      0.60      0.60       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print skmetrics.classification_report(y_true=yt, y_pred=yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60256410256410253"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yt, yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V Stop + LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_merged = merge_attributes(train, X_train_word_average)\n",
    "test_merged = merge_attributes(test, X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "force = False\n",
    "model_trainer = RandomizedSearchCV(\n",
    "    n_iter=1, \n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_distributions={\n",
    "        \"criterion\": [\"gini\"],\n",
    "        \"n_estimators\": [1000],\n",
    "        \"max_features\": [\"log2\"],\n",
    "        \"max_depth\": [None],\n",
    "        \"bootstrap\": [True],\n",
    "        \"oob_score\": [True],\n",
    "        \"class_weight\": [\"balanced\"],\n",
    "        \"random_state\": [42]\n",
    "    },\n",
    "    scoring=\"f1\",\n",
    "    verbose=True,\n",
    "    refit=True,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.4 s, sys: 87.9 ms, total: 5.49 s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_trainer.fit(train_merged, train_w2v_data['extraversion_m'])\n",
    "model = model_trainer.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp = model.predict(test_merged)\n",
    "yt = test_w2v_data['extraversion_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y_pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y_pred    0   1\n",
       "y_true         \n",
       "0       104  38\n",
       "1        84  86"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    index=pd.Index([0, 1], name=\"y_true\"),\n",
    "    columns=pd.Index([0, 1], name=\"y_pred\"),\n",
    "    data=skmetrics.confusion_matrix(y_true=yt, y_pred=yp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.73      0.63       142\n",
      "          1       0.69      0.51      0.59       170\n",
      "\n",
      "avg / total       0.63      0.61      0.61       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print skmetrics.classification_report(y_true=yt, y_pred=yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60897435897435892"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yt, yp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
