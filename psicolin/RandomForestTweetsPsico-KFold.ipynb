{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-27 16:11:54,618 : INFO : 'pattern' package found; tag filters are available for English\n",
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "import os, codecs\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, 186)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('~/personality-normalized-word2vec-norm.csv', encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psico = pd.read_csv('/home/bahbbc/Downloads/psycholinguistic_properties/BP.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Simplified grammatical category</th>\n",
       "      <th>Concretenes</th>\n",
       "      <th>Subjective Frequency</th>\n",
       "      <th>Imagery</th>\n",
       "      <th>AoA</th>\n",
       "      <th>Log frequency</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abafado</td>\n",
       "      <td>a</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.96</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abafador</td>\n",
       "      <td>a</td>\n",
       "      <td>5.73</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.10</td>\n",
       "      <td>7.25</td>\n",
       "      <td>4.41</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abalado</td>\n",
       "      <td>a</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.59</td>\n",
       "      <td>7.62</td>\n",
       "      <td>8.15</td>\n",
       "      <td>3450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abalizado</td>\n",
       "      <td>a</td>\n",
       "      <td>3.88</td>\n",
       "      <td>2.58</td>\n",
       "      <td>3.56</td>\n",
       "      <td>7.59</td>\n",
       "      <td>4.36</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonado</td>\n",
       "      <td>a</td>\n",
       "      <td>3.68</td>\n",
       "      <td>4.16</td>\n",
       "      <td>3.97</td>\n",
       "      <td>5.24</td>\n",
       "      <td>9.75</td>\n",
       "      <td>17183.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word Simplified grammatical category  Concretenes  \\\n",
       "0     abafado                               a         3.47   \n",
       "1    abafador                               a         5.73   \n",
       "2     abalado                               a         2.61   \n",
       "3   abalizado                               a         3.88   \n",
       "4  abandonado                               a         3.68   \n",
       "\n",
       "   Subjective Frequency  Imagery   AoA  Log frequency  Frequency  \n",
       "0                  3.92     3.96  5.28           7.11     1220.0  \n",
       "1                  2.84     5.10  7.25           4.41       82.0  \n",
       "2                  4.12     3.59  7.62           8.15     3450.0  \n",
       "3                  2.58     3.56  7.59           4.36       78.0  \n",
       "4                  4.16     3.97  5.24           9.75    17183.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concreteness = dict(zip(psico.Word, psico.Concretenes))\n",
    "subjective_freq = dict(zip(psico.Word, psico['Subjective Frequency']))\n",
    "imagery = dict(zip(psico.Word, psico.Imagery))\n",
    "aoa =  dict(zip(psico.Word, psico.AoA))\n",
    "log_freq = dict(zip(psico.Word, psico['Log frequency']))\n",
    "freq = dict(zip(psico.Word, psico.Frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, psico_dict):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = 0.\n",
    "    #\n",
    "    nwords = 0.\n",
    "    \n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if (word in psico_dict):\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = featureVec + psico_dict[word]\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = featureVec / nwords\n",
    "    else:\n",
    "        print 'sorry, empty...'\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, psico_dict):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews)),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        #Print a status message every 1000th review\n",
    "        if counter%100. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "        #Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, psico_dict)\n",
    "        #Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    if text is np.nan:\n",
    "        return []\n",
    "    for sent in nltk.sent_tokenize(text, language='portuguese'):\n",
    "        for word in nltk.word_tokenize(sent, language='portuguese'):\n",
    "            word = word.lower()\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_tokenized = df.apply(lambda r: w2v_tokenize_text(r['formatted_text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = pd.isnull(df.formatted_text)\n",
    "df.loc[index, 'formatted_text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concreteness = dict(zip(psico.Word, psico.Concretenes))\n",
    "subjective_freq = dict(zip(psico.Word, psico['Subjective Frequency']))\n",
    "imagery = dict(zip(psico.Word, psico.Imagery))\n",
    "aoa =  dict(zip(psico.Word, psico.AoA))\n",
    "log_freq = dict(zip(psico.Word, psico['Log frequency']))\n",
    "freq = dict(zip(psico.Word, psico.Frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1039\n",
      "Review 100 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 200 of 1039\n",
      "sorry, empty...\n",
      "Review 300 of 1039\n",
      "sorry, empty...\n",
      "Review 400 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 500 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 600 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 700 of 1039\n",
      "Review 800 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 900 of 1039\n",
      "sorry, empty...\n",
      "Review 1000 of 1039\n",
      "Review 0 of 1039\n",
      "Review 100 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 200 of 1039\n",
      "sorry, empty...\n",
      "Review 300 of 1039\n",
      "sorry, empty...\n",
      "Review 400 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 500 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 600 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 700 of 1039\n",
      "Review 800 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 900 of 1039\n",
      "sorry, empty...\n",
      "Review 1000 of 1039\n",
      "Review 0 of 1039\n",
      "Review 100 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 200 of 1039\n",
      "sorry, empty...\n",
      "Review 300 of 1039\n",
      "sorry, empty...\n",
      "Review 400 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 500 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 600 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 700 of 1039\n",
      "Review 800 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 900 of 1039\n",
      "sorry, empty...\n",
      "Review 1000 of 1039\n",
      "Review 0 of 1039\n",
      "Review 100 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 200 of 1039\n",
      "sorry, empty...\n",
      "Review 300 of 1039\n",
      "sorry, empty...\n",
      "Review 400 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 500 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 600 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 700 of 1039\n",
      "Review 800 of 1039\n",
      "sorry, empty...\n",
      "sorry, empty...\n",
      "Review 900 of 1039\n",
      "sorry, empty...\n",
      "Review 1000 of 1039\n"
     ]
    }
   ],
   "source": [
    "conc_trainDataVecs = getAvgFeatureVecs( data_tokenized, concreteness )\n",
    "sub_trainDataVecs = getAvgFeatureVecs( data_tokenized, subjective_freq )\n",
    "ima_trainDataVecs = getAvgFeatureVecs( data_tokenized, imagery )\n",
    "aoa_trainDataVecs = getAvgFeatureVecs( data_tokenized, aoa )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.vstack((conc_trainDataVecs,sub_trainDataVecs, ima_trainDataVecs, aoa_trainDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=42, criterion= \"gini\", class_weight=\"balanced\", n_estimators=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, data, df['extraversion_ober_2'], cv=10, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.27 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreableness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, data, df['agreeableness_ober_2'], cv=10, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.28 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conciousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, data, df['conscientiousness_ober_2'], cv=10, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.28 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, data, df['neuroticism_ober_2'], cv=10, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.29 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, data, df['openness_ober_2'], cv=10, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.24 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
