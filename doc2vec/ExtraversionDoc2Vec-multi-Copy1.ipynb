{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-07 23:57:40,464 : INFO : 'pattern' package found; tag filters are available for English\n",
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "import os, codecs\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics as skmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify model with personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/personality-normalized-word2vec-norm.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, 186)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    529\n",
       "1    510\n",
       "Name: extraversion_m, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.extraversion_m.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_w2v_data, test_w2v_data = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    target_names = ['no', 'yes']\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    print('accuracy %s' % accuracy_score(target, predictions))\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(vectorizer, classifier, data):\n",
    "    data_features = vectorizer.transform(data['formatted_text'])\n",
    "    predictions = classifier.predict(data_features)\n",
    "    target = int(data['extraversion'])\n",
    "    evaluate_prediction(predictions, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    if text is np.nan:\n",
    "        return []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tagged = train_w2v_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['formatted_text']), tags=[r.extraversion]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tagged = test_w2v_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['formatted_text']), tags=[r.extraversion]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-07 23:58:25,413 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-07-07 23:58:25,414 : INFO : collecting all words and their counts\n",
      "2017-07-07 23:58:25,415 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-07-07 23:58:25,813 : INFO : collected 43446 word types and 31 unique tags from a corpus of 727 examples and 1472087 words\n",
      "2017-07-07 23:58:25,814 : INFO : Loading a fresh vocabulary\n",
      "2017-07-07 23:58:26,046 : INFO : min_count=5 retains 13634 unique words (31% of original 43446, drops 29812)\n",
      "2017-07-07 23:58:26,046 : INFO : min_count=5 leaves 1421890 word corpus (96% of original 1472087, drops 50197)\n",
      "2017-07-07 23:58:26,095 : INFO : deleting the raw counts dictionary of 43446 items\n",
      "2017-07-07 23:58:26,097 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2017-07-07 23:58:26,099 : INFO : downsampling leaves estimated 1039507 word corpus (73.1% of prior 1421890)\n",
      "2017-07-07 23:58:26,100 : INFO : estimated required memory for 13634 words and 5 dimensions: 7369180 bytes\n",
      "2017-07-07 23:58:26,156 : INFO : resetting layer weights\n",
      "2017-07-07 23:58:26,301 : INFO : training model with 1 workers on 13634 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-07-07 23:58:26,302 : INFO : expecting 727 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-07-07 23:58:27,316 : INFO : PROGRESS: at 2.45% examples, 533115 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:28,324 : INFO : PROGRESS: at 5.23% examples, 539828 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:29,327 : INFO : PROGRESS: at 8.49% examples, 584447 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:30,342 : INFO : PROGRESS: at 11.60% examples, 598104 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:31,350 : INFO : PROGRESS: at 14.36% examples, 591023 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:32,356 : INFO : PROGRESS: at 16.97% examples, 584829 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:33,360 : INFO : PROGRESS: at 19.95% examples, 584919 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:34,360 : INFO : PROGRESS: at 22.63% examples, 585287 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:35,363 : INFO : PROGRESS: at 25.23% examples, 577507 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:36,367 : INFO : PROGRESS: at 27.86% examples, 575703 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:37,370 : INFO : PROGRESS: at 30.96% examples, 579436 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:38,375 : INFO : PROGRESS: at 34.13% examples, 585766 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:39,380 : INFO : PROGRESS: at 37.26% examples, 592564 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:40,383 : INFO : PROGRESS: at 40.83% examples, 600703 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:41,388 : INFO : PROGRESS: at 44.31% examples, 608661 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:42,390 : INFO : PROGRESS: at 47.50% examples, 612964 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:43,399 : INFO : PROGRESS: at 50.14% examples, 607322 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:44,400 : INFO : PROGRESS: at 52.68% examples, 604233 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:45,412 : INFO : PROGRESS: at 55.36% examples, 600031 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:46,413 : INFO : PROGRESS: at 58.20% examples, 600400 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:47,417 : INFO : PROGRESS: at 61.51% examples, 603235 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:48,417 : INFO : PROGRESS: at 63.87% examples, 598605 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:49,422 : INFO : PROGRESS: at 66.38% examples, 594414 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:50,425 : INFO : PROGRESS: at 68.87% examples, 591657 words/s, in_qsize 2, out_qsize 0\n",
      "2017-07-07 23:58:51,430 : INFO : PROGRESS: at 71.81% examples, 592429 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:52,432 : INFO : PROGRESS: at 75.12% examples, 595158 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:53,434 : INFO : PROGRESS: at 78.45% examples, 599006 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:54,437 : INFO : PROGRESS: at 81.46% examples, 599398 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:55,443 : INFO : PROGRESS: at 84.97% examples, 603474 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:56,447 : INFO : PROGRESS: at 88.38% examples, 607563 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:57,457 : INFO : PROGRESS: at 91.44% examples, 607565 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:58,465 : INFO : PROGRESS: at 94.62% examples, 609265 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:58:59,474 : INFO : PROGRESS: at 97.77% examples, 610832 words/s, in_qsize 1, out_qsize 0\n",
      "2017-07-07 23:59:00,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-07 23:59:00,106 : INFO : training on 29441740 raw words (20698345 effective words) took 33.8s, 612348 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.9 s, sys: 236 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainsent = train_tagged.values\n",
    "testsent = test_tagged.values\n",
    "\n",
    "# simple gensim doc2vec api\n",
    "doc2vec_model = Doc2Vec(trainsent, workers=1, size=5, iter=20, dm=1)\n",
    "\n",
    "train_targets, train_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in trainsent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 1422\n",
    "\n",
    "doc2vec_model.seed = seed\n",
    "doc2vec_model.random = random.RandomState(seed)\n",
    "\n",
    "\n",
    "test_targets, test_regressors = zip(\n",
    "    *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in testsent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "force = False\n",
    "model_trainer = RandomizedSearchCV(\n",
    "    n_iter=1, \n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    param_distributions={\n",
    "        \"max_features\": [\"log2\"],\n",
    "        \"random_state\": [42],\n",
    "        \"criterion\":['mse']\n",
    "    },\n",
    "    verbose=True,\n",
    "    refit=True,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 316 ms, sys: 59.6 ms, total: 375 ms\n",
      "Wall time: 481 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_trainer.fit(train_regressors, train_targets)\n",
    "model = model_trainer.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp = model.predict(test_regressors)\n",
    "yt = test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2166769764957264"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmetrics.mean_squared_error(yt, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1030308139375464"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmetrics.mean_squared_error(yt, yp)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.69296344231725038"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmetrics.r2_score(yt, yp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
